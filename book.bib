@Manual{rmarkdown2021,
  title = {rmarkdown: Dynamic Documents for R},
  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},
  year = {2021},
  note = {R package version 2.10},
  url = {https://github.com/rstudio/rmarkdown},
}

@Book{Xie2018,
  title = {R Markdown: The Definitive Guide},
  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2018},
  note = {ISBN 9781138359338},
  url = {https://bookdown.org/yihui/rmarkdown},
}

@Book{Xie2020,
  title = {R Markdown Cookbook},
  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2020},
  note = {ISBN 9780367563837},
  url = {https://bookdown.org/yihui/rmarkdown-cookbook},
}

@misc{washingtonpost2023,
  author = {Kevin Schaul and Szu Yu Chen and Nitasha Tiku},
  title = {Inside the secret list of websites that make AI like ChatGPT sound smart},
  url = {https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/},
  year = {2023},
  howpublished = {Online},
  note = {Accessed: April 26, 2023}
}

@article{belenguer_ai_2022,
	title = {{AI} bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry},
	volume = {2},
	issn = {2730-5953},
	shorttitle = {{AI} bias},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8830968/},
	doi = {10.1007/s43681-022-00138-8},
	abstract = {A new and unorthodox approach to deal with discriminatory bias in Artificial Intelligence is needed. As it is explored in detail, the current literature is a dichotomy with studies originating from the contrasting fields of study of either philosophy and sociology or data science and programming. It is suggested that there is a need instead for an integration of both academic approaches, and needs to be machine-centric rather than human-centric applied with a deep understanding of societal and individual prejudices. This article is a novel approach developed into a framework of action: a bias impact assessment to raise awareness of bias and why, a clear set of methodologies as shown in a table comparing with the four stages of pharmaceutical trials, and a summary flowchart. Finally, this study concludes the need for a transnational independent body with enough power to guarantee the implementation of those solutions.},
	number = {4},
	urldate = {2023-05-02},
	journal = {Ai and Ethics},
	author = {Belenguer, Lorenzo},
	year = {2022},
	pmid = {35194591},
	pmcid = {PMC8830968},
	pages = {771--787}
}

@misc{wikipedia_chatbot_2023,
	title = {Chatbot},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Chatbot&oldid=1153723735},
	abstract = {A chatbot (originally chatterbot) is a software application that aims to mimic human conversation through text or voice interactions, typically online. The term "ChatterBot" was originally coined by Michael Mauldin (creator of the first Verbot) in 1994 to describe conversational programs. Modern chatbots are artificial intelligence (AI) systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. Such technologies often utilize aspects of deep learning and natural language processing. 
Recently this field has gained widespread attention due to the popularity of OpenAI's ChatGPT, followed by alternatives such as Microsoft's Bing Chat (which uses OpenAI's GPT-4) and Google's Bard. Such examples reflect the recent practice of such products being built based upon broad foundational large language models that get fine-tuned so as to target specific tasks or applications (i.e. simulating human conversation, in the case of chatbots). Chatbots can also be designed or customized to further target even more specific situations and/or particular subject-matter domains.A major area where chatbots have long been used is in customer service and support, such as with various sorts of virtual assistants. Recently, companies spanning various industries have begun using the latest generative AI technologies to power more advanced developments in such areas.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Wikipedia},
	month = may,
	year = {2023},
	note = {Page Version ID: 1153723735},
}

@inproceedings{abdulla2022chatbots,
  title={Chatbots Development Using Natural Language Processing: A Review},
  author={Abdulla, Hussam and Eltahir, Asim Mohammed and Alwahaishi, Saleh and Saghair, Khalifa and Platos, Jan and Snasel, Vaclav},
  booktitle={2022 26th International Conference on Circuits, Systems, Communications and Computers (CSCC)},
  pages={122--128},
  year={2022},
  organization={IEEE}
}

@article{caldarini2022literature,
  title={A literature survey of recent advances in chatbots},
  author={Caldarini, Guendalina and Jaf, Sardar and McGarry, Kenneth},
  journal={Information},
  volume={13},
  number={1},
  pages={41},
  year={2022},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{cahn2017chatbot,
  title={CHATBOT: Architecture, design, \& development},
  author={Cahn, Jack},
  journal={University of Pennsylvania School of Engineering and Applied Science Department of Computer and Information Science},
  year={2017}
}

@misc{wikipedia_natural_2023,
	title = {Natural language processing},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1148327592},
	abstract = {Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The goal is a computer capable of "understanding" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Wikipedia},
	month = apr,
	year = {2023},
	note = {Page Version ID: 1148327592},
}


@misc{wikipedia_large_language_2023,
	title = {Large language model},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1153821175},
	abstract = {A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.Though the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more. LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning). The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.Though trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language. In addition, large language models demonstrate considerable general knowledge about the world, and are able to "memorize" a great quantity of facts during training.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Wikipedia},
	month = may,
	year = {2023},
	note = {Page Version ID: 1153821175},
}


@misc{wikipedia_deep_learning_2023,
	title = {Deep learning},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Deep_learning&oldid=1152717003},
	abstract = {Deep learning is part of a broader family of machine learning methods, which is based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.The adjective "deep" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Wikipedia},
	month = may,
	year = {2023},
	note = {Page Version ID: 1152717003},
}

@misc{schaul_inside_nodate,
	title = {Inside the secret list of websites that make {AI} like {ChatGPT} sound smart},
	url = {https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/},
	abstract = {An analysis of a chatbot data set by The Washington Post reveals the proprietary, personal, and often offensive websites that go into an AI’s training data.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Washington Post},
	author = {Schaul, Kevin and Chen, Szu Yu and Tiku, Nitasha}
}

